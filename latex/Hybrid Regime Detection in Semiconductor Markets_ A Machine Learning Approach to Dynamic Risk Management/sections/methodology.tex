\section{Methodology}

\subsection{1. Data \& Features}

We construct our asset universe from major semiconductor equities and sector ETFs, including \texttt{SMH}, \texttt{SOXX}, \texttt{NVDA}, \texttt{AMD}, \texttt{TSM}, \texttt{INTC}, \texttt{QCOM}, and \texttt{AVGO}. The sample period spans from January 1, 2019 to January 1, 2024, at daily frequency.

We engineer features spanning multiple domains:
\begin{itemize}
    \item \textbf{Price-based}: Returns $r_t$, log-returns, realized volatility $\hat{\sigma}_{t,L}$, exponential volatility, $N$-window momentum $M_{t,N}$, rolling skew/kurtosis.
    \item \textbf{Technical indicators}: RSI$_t$, MACD$_t$, Bollinger band position $\mathrm{BB}_t$, Williams~\%R$_t$, on-balance volume.
    \item \textbf{Sector-specific}: Semiconductor PMI proxy, memory vs. logic spread $\Delta_{\mathrm{Mem/Log}}$, equipment vs. design volatility.
    \item \textbf{Macro proxies}: VIX$^{\text{(S\&P500)}}_t$, yield spreads $\Delta_{\mathrm{YS}}$, DXY/dollar index.
\end{itemize}

Missing values are forward-filled, extreme observations winsorized at 1st/99th percentiles, and all features are standardized:
\begin{equation}
x^*_{i,t} = \frac{x_{i,t} - \hat{\mu}_i}{\hat{\sigma}_i}
\end{equation}
where $\hat{\mu}_i, \hat{\sigma}_i$ denote robust mean and standard deviation for feature $i$.

\subsection{2. Regime Detection Models}

We employ a hybrid regime detection architecture: a Gaussian HMM initialized by KMeans and a deep LSTM classifier.

\subsubsection*{Hidden Markov Model (HMM)}

Given observed feature vectors $X = \{x_t\}$, we estimate regime posteriors for $K$ hidden states via
\begin{align}
P(z_t = k | x_{1:t}) &= \frac{P(x_t | z_t = k) \sum_j P(z_t = k | z_{t-1} = j) P(z_{t-1}=j | x_{1:t-1})}{P(x_t | x_{1:t-1})} \\
A_{ij} &= P(z_t = j | z_{t-1} = i) \nonumber
\end{align}
where $z_t$ is the latent regime at time $t$. Each regime conditional distribution $P(x_t | z_t = k)$ is modeled as a multivariate Gaussian:
\begin{equation}
P(x_t | z_t = k) = \mathcal{N}(x_t\,|\, \mu_k, \Sigma_k)
\end{equation}

\subsubsection*{Deep LSTM Classifier}

The LSTM receives $(x_{t-\ell+1}, \ldots, x_t)$ as input and predicts regime assignments $\tilde{P}_{\mathrm{LSTM}}(z_t = k)$. It is supervised by the HMM’s initial labels, with architecture:
\begin{align}
h_t &= \mathrm{LSTM}(x_{t-\ell+1}, ..., x_t) \\
\tilde{P}_{\mathrm{LSTM}}(z_t = k) &= \mathrm{Softmax}(W h_t + b)
\end{align}
Bidirectional layers, attention mechanisms, and dropout are used to improve sequential learning and generalization.

\subsubsection*{Smoothing and Uncertainty Thresholds}

Final regime assignments are smoothed with a rolling mode filter to suppress noise:
\begin{equation}
\hat{z}_t = \mathrm{mode} ( \{ z_{t-w}, \ldots, z_t \} )
\end{equation}
Confidence is given by max-probability across posterior:
\begin{equation}
C_t = \max_k P_{\mathrm{fused}}(z_t = k)
\end{equation}
Regime assignments with $C_t < \tau$ are flagged as uncertain.

\subsection{3. Model Fusion / Ensemble}

We fuse predictions with Bayesian model averaging weighted by entropy:
\begin{align}
\mathcal{H}(P) &= - \sum_k P_k \log P_k \\
w_{\mathrm{HMM}}(t) &= \frac{1 - \mathcal{H}(P_{\mathrm{HMM}}(t))}{1 - \mathcal{H}(P_{\mathrm{HMM}}(t)) + 1 - \mathcal{H}(P_{\mathrm{LSTM}}(t))}\\
P_{\text{fused}}(t) &= w_{\text{HMM}}(t) P_{\text{HMM}}(t) + (1 - w_{\text{HMM}}(t)) P_{\text{LSTM}}(t)
\end{align}

The regime label $\hat{z}_t = \arg \max_k P_{\text{fused},k}(t)$. Uncertainty is inherited from entropy $\mathcal{H}(P_{\text{fused}}(t))$.

\subsection{4. Trading Signal Generation}

We construct a composite signal as a weighted sum of normalized feature-based scores:
\begin{align}
f^*_{i,t} &= \frac{f_{i,t} - \mu_i}{\sigma_i} \\
s_t &= \sum_{i=1}^M \alpha^{(\hat{z}_t)}_i f^*_{i,t}
\end{align}
where component weights $\boldsymbol{\alpha}^{(\hat{z}_t)}$ are regime-specific and calibrated using meta-learning (e.g., RidgeCV or Lasso):
\begin{equation}
\boldsymbol{\alpha}^{(r)} = \arg\min_{\boldsymbol{\alpha}} \Big\| \mathbf{y} - \sum_{i} \alpha_i f^*_i \Big\|_2^2 + \lambda \|\boldsymbol{\alpha}\|^2_2
\end{equation}

Signal clipping and risk overlays ensure risk control:
\begin{equation}
\tilde{s}_t = \mathrm{clip}(s_t, -1, 1)
\end{equation}
and
\begin{equation}
\bar{s}_t = g(R_{\max}, r, DD_t) \cdot \tilde{s}_t
\end{equation}
where $g$ is a risk overlay function (e.g., scaling by recent drawdown, volatility, or VaR breaches).

Signal quality is validated via forward rolling correlations:
\begin{equation}
\mathrm{Corr}(s_{t-\tau:t}, r_{t-\tau:t})
\end{equation}

\subsection{5. Risk Management Strategy}

We compute standard risk measures and design regime-adaptive position sizing:
\begin{align}
\text{VaR}_\alpha &= \inf \{ x : P(L > x) \leq 1 - \alpha \} \\
\text{ES}_\alpha &= \mathbb{E}[L \mid L > \text{VaR}_\alpha] \\
\mathrm{vol}_{t,T} &= \sqrt{ \frac{1}{T} \sum_{i=0}^{T-1} (r_{t-i} - \bar{r}_{t,T})^2 }
\end{align}

Position sizes are scaled inversely to realized volatility and drawdown:
\begin{equation}
\pi_t = \kappa \frac{\tilde{s}_t}{\mathrm{vol}_{t, T} + \epsilon} \cdot \mathbf{1}_{\{ C_t \geq \tau \}}
\end{equation}
where $\kappa$ is a leverage factor, and $\mathbf{1}_{\{ C_t \geq \tau \}}$ is an indicator for regime confidence.

Portfolio-level risk rules include hard stop-loss, dynamic stop-outs based on current/max drawdown, and notional caps.

\subsection{6. Backtesting Framework}

We assess all strategies using rolling and expanding window backtests with realistic assumptions:
\begin{itemize}
  \item Transaction costs $c$ and slippage applied as:
  \begin{equation}
  P_{t+1} = P_t (1 + r_{t+1}) - c \cdot | \pi_{t+1} - \pi_{t} | \cdot P_t
  \end{equation}
  \item Walk-forward training/validation cycles prevent lookahead bias.
  \item Metrics include annualized return, Sharpe, Sortino, Calmar, max drawdown, turnover, and realized risk reductions.
\end{itemize}

\subsection{7. Scenario Analysis (Monte Carlo)}

To assess tail risk, we simulate $N$ alternative paths via block bootstrap or parametric models (optionally regime-conditional):
\begin{align}
\{ r_{t}^{(n)} \}_{n=1}^N &\sim \mathrm{Bootstrap}( \{ r_t \} ) \\
\text{Portfolio risk statistics:}\quad &\text{VaR},\; \text{ES},\; \text{maxDD} \text{ computed $\forall n$}
\end{align}
Stress scenarios (e.g., memory sector crash, policy shock) can be implemented by perturbing regime transition probabilities or conditional volatility.

\subsection{8. Rationale for Key Model Choices}

LSTM networks are employed to capture long-horizon, nonlinear dependencies associated with regime transitions in asset returns, outperforming GRUs on both forecast stability and robust segmentation in our experiments.

A three-state HMM is selected via AIC/BIC and based on economic interpretability, mapping directly to low, medium, and high risk/drawdown regimes commonly observed in the semiconductor sector; two-state models underfit, while models with $K > 3$ tended to overfit and fragment the risk landscape impractically.

Separation of regime detection and signal generation, with entropy-based Bayesian fusion for model uncertainty, enables full transparency. This modular ensemble approach, rather than a monolithic black box, is motivated by practical risk management imperatives and aligns with requirements from institutional allocators and risk oversight committees.

\subsection{9. Overall System Design}

The architecture is end-to-end modular with reproducible data handling (\texttt{pandas}/\texttt{numpy}). Each block—feature pipeline, regime detection, fusion, signal generation, risk overlay, backtest, evaluation—is independently configurable and extensible.
\begin{center}
\begin{tabular}{c}
Data $\rightarrow$ Feature Engineering $\rightarrow$ HMM \& LSTM $\rightarrow$ Entropy-weighted Ensemble $\rightarrow$ Signal Generation $\rightarrow$ Risk Management $\rightarrow$ Backtest $\rightarrow$ Evaluation
\end{tabular}
\end{center}
